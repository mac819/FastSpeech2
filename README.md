# AudioML - FastSpeech2 Implementation

<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>

This repository contains an implementation of Microsoft's **FastSpeech 2: Fast and High-Quality End-to-End Text to Speech** with various improvements and optimizations for better training stability and performance.

## üöÄ Features

- **FastSpeech2 Implementation**: Complete end-to-end text-to-speech model
- **Improved Training Stability**: Fixed learning rate calculation and mel-spectrogram normalization
- **Enhanced Monitoring**: Comprehensive logging with mel-spectrogram debugging
- **Modular Architecture**: Clean separation of components (encoder, variance adaptor, decoder)
- **Data Processing Pipeline**: Complete preprocessing for LJSpeech dataset

## üìÅ Project Organization

```
‚îú‚îÄ‚îÄ LICENSE                    <- Open-source license
‚îú‚îÄ‚îÄ Makefile                   <- Makefile with convenience commands
‚îú‚îÄ‚îÄ README.md                  <- This file
‚îú‚îÄ‚îÄ pyproject.toml            <- Project configuration and package metadata
‚îú‚îÄ‚îÄ requirements.txt           <- Python dependencies
‚îú‚îÄ‚îÄ config.yaml               <- Model and training configuration
‚îÇ
‚îú‚îÄ‚îÄ data/                     <- Data directory
‚îÇ   ‚îú‚îÄ‚îÄ external/             <- Data from third party sources
‚îÇ   ‚îú‚îÄ‚îÄ interim/              <- Intermediate data (text transcripts)
‚îÇ   ‚îú‚îÄ‚îÄ processed/            <- Processed features (mel-spectrograms, pitch, energy)
‚îÇ   ‚îî‚îÄ‚îÄ raw/                  <- Original LJSpeech dataset
‚îÇ
‚îú‚îÄ‚îÄ docs/                     <- Documentation
‚îÇ   ‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ getting-started.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.md
‚îÇ   ‚îî‚îÄ‚îÄ mkdocs.yml
‚îÇ
‚îú‚îÄ‚îÄ models/                   <- Trained model checkpoints
‚îú‚îÄ‚îÄ notebooks/                <- Jupyter notebooks for exploration
‚îÇ   ‚îú‚îÄ‚îÄ alignment_data_creation.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ train.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ FastSpeech2.ipynb
‚îÇ
‚îú‚îÄ‚îÄ references/               <- Reference materials and images
‚îÇ   ‚îî‚îÄ‚îÄ images/
‚îÇ       ‚îú‚îÄ‚îÄ fastspeech2_model.png
‚îÇ       ‚îî‚îÄ‚îÄ fastspeech2.drawio.png
‚îÇ
‚îú‚îÄ‚îÄ reports/                  <- Generated analysis and figures
‚îÇ   ‚îî‚îÄ‚îÄ figures/
‚îÇ
‚îî‚îÄ‚îÄ audioml/                 <- Main source code
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ config.yaml           <- Model configuration
    ‚îú‚îÄ‚îÄ train.py              <- Training script with improvements
    ‚îú‚îÄ‚îÄ synthesize.py          <- Inference script
    ‚îÇ
    ‚îú‚îÄ‚îÄ dataset/              <- Data loading and processing
    ‚îÇ   ‚îî‚îÄ‚îÄ feature_dataset.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ fastspeech/           <- FastSpeech2 model implementation
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ model.py          <- Main model architecture
    ‚îÇ   ‚îú‚îÄ‚îÄ transformer.py    <- Transformer blocks
    ‚îÇ   ‚îú‚îÄ‚îÄ speech_feature_predictor.py  <- Duration, pitch, energy predictors
    ‚îÇ   ‚îî‚îÄ‚îÄ utils.py          <- Utility functions
    ‚îÇ
    ‚îú‚îÄ‚îÄ processing/           <- Data preprocessing
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ feature_compute.py    <- Feature extraction
    ‚îÇ   ‚îú‚îÄ‚îÄ prepare_ljspeech_files.py
    ‚îÇ   ‚îî‚îÄ‚îÄ text_speech_alignment.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ text/                 <- Text processing
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ cleaners.py
    ‚îÇ   ‚îú‚îÄ‚îÄ cmudict.py
    ‚îÇ   ‚îú‚îÄ‚îÄ numbers.py
    ‚îÇ   ‚îî‚îÄ‚îÄ symbols.py
    ‚îÇ
    ‚îî‚îÄ‚îÄ runs/                 <- Training logs and outputs
        ‚îî‚îÄ‚îÄ fastspeech2/
```

## üèóÔ∏è Model Architecture

The FastSpeech2 implementation consists of three main components:

### 1. **Encoder**
- Text embedding with positional encoding
- Multi-layer transformer blocks
- Processes input text tokens

### 2. **Variance Adaptor**
- **Duration Predictor**: Predicts phoneme durations
- **Length Regulator**: Expands encoder outputs based on durations
- **Pitch Predictor**: Predicts pitch contours using CWT
- **Energy Predictor**: Predicts frame-level energy

### 3. **Decoder**
- Multi-layer transformer blocks
- Generates mel-spectrograms
- Outputs log-transformed mel-spectrograms

## üöÄ Quick Start

### **1. Setup Environment**
```bash
# Clone the repository
git clone <repository-url>
cd audio

# Install dependencies
pip install -r requirements.txt
```

### **2. Prepare Data**
```bash
# Download LJSpeech dataset to data/raw/
# Run preprocessing
python audioml/processing/prepare_ljspeech_files.py
python audioml/processing/feature_compute.py
```

### **3. Train Model**
```bash
# Start training
python audioml/train.py
```

### **4. Monitor Training**
- **Console**: Real-time loss and mel-spectrogram statistics
- **Log file**: `audioml/training_log.txt` with detailed metrics
- **TensorBoard**: `runs/fastspeech2/` for visualizations

## üìä Training Configuration

### **Optimizer Settings**
```yaml
learning_rate: 0.0001          # Fixed from 0.0625 bug
warm_up_steps: 8000           # Better warmup
anneal_steps: [200000, 400000, 600000]  # Gradual decay
weight_decay: 0.001           # Reduced from 0.01
eps: 1e-8                     # Better numerical stability
```

### **Model Architecture**
```yaml
encoder:
  n_fft_layers: 4
  embedding_layer:
    dims: 256
  fft:
    d_in: 256
    d_out: 256
    n_head: 2

decoder:
  n_fft_layers: 4
  n_mels: 128
  fft:
    d_in: 384
    d_out: 384
```

## üìà Expected Results

With the improvements, you should see:

1. **Stable Training**: No gradient explosions
2. **Better Convergence**: Loss decreases steadily
3. **Meaningful Outputs**: Mel-spectrograms with proper patterns
4. **Improved Quality**: Better speech synthesis results

## üìù Logging and Monitoring

### **Training Log Format**
```csv
step,total_loss,mel_loss,duration_loss,pitch_spec_loss,pitch_mean_loss,pitch_std_loss,energy_loss,pred_mel_min,pred_mel_max,pred_mel_mean,target_mel_min,target_mel_max,target_mel_mean
100,6.2345,0.1234,0.5678,0.2345,0.3456,0.4567,0.6789,-8.1234,2.5678,-2.3456,-7.8901,3.4567,-1.2345
```

### **Expected Value Ranges**
- **Log-transformed mel-spectrograms**: -10 to +10
- **Predicted outputs**: Should match target ranges
- **Loss values**: Should decrease over time


### **Debug Commands**
```bash
# Check mel-spectrogram ranges
python test_normalization.py

# Monitor training logs
tail -f audioml/training_log.txt

# View TensorBoard
tensorboard --logdir runs/fastspeech2
```

## üìö References

- **Original Paper**: [FastSpeech 2: Fast and High-Quality End-to-End Text to Speech](https://arxiv.org/abs/2006.04558)
- **Reference Implementation**: [ming024/FastSpeech2](https://github.com/ming024/FastSpeech2)
- **Dataset**: [LJSpeech](https://keithito.com/LJ-Speech-Dataset/)

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**Note**: This implementation includes several improvements over the original FastSpeech2 to address common training issues and improve stability.